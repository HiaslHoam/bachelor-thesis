As already mentioned, deep learning has really taken image classification to another level in the recent years. One could assume that it is as easy as using many convolutional layers in repeat to achieve this. However, training a neural network with a high depth was neither effective, nor more accurate. The deeper the input signal is forwarded into the network, the higher the chance for actually losing the signal is, leaving only noise instead of a signal to the dense layers. AlexNet \citep{Krizhevsky_2012} was one of the first deep neural networks that could achieve higher accuracies than shallow networks, benefiting from training on a GPU rather than a CPU. It was outperformed by VGG \citep{Simonyan_2015} a few years later. Big improvements could be seen with the introduction of ResNet \citep{He_2016} using residuals to overcome limitations posed by a deeper architecture. A more recent deep CNN called EfficientNet \citep{tan2020efficientnet} tried different scaling techniques to achieve even higher accuracies. I want to take a closer look at these architectures before feeding them with galaxy cluster data in \autoref{training}.